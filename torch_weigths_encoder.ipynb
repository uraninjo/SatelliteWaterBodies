{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch with Pretrained Weigths and Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasik yüklemelerimizi yapıyoruz ve cuda ile GPU'nun aktif olup olmadığını kontrol ediyoruz..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade opencv-contrib-python\n",
    "!pip install segmentation-models-pytorch \n",
    "!pip install -U git+https://github.com/albumentations-team/albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import HorizontalFlip, VerticalFlip, Rotate\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "size = (256, 256)\n",
    "\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veriyi düzgün hata vermeyecek şekilde modelimize göndermeden önce düzenlememiz ve modelimize uygun bir şekle getirmemiz gerekiyor.\n",
    "\n",
    "Bunun için kendimize bir sınıf oluşturup içine olmasını istediğimiz fonksiyonları yazıyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, images_path, masks_path):\n",
    "        super().__init__()\n",
    "\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.len = len(images_path)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(0.1,0.1,0.1),\n",
    "        ])\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images_path[idx])\n",
    "        img = self.transform(img)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = img/255.0\n",
    "        img = torch.tensor(img)\n",
    "\n",
    "        mask = Image.open(self.masks_path[idx]).convert('L')\n",
    "        mask = self.transform(mask)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        mask = mask/255.0\n",
    "        mask = torch.tensor(mask)\n",
    "\n",
    "        return img, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verimizi glob ile X-Y olarak yüklüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted(glob.glob('C:/Users/mehmu/Desktop/PyProjects/Projects/github repo/SatelliteWaterBodies/Water Bodies Dataset/Images/*'))\n",
    "y = sorted(glob.glob('C:/Users/mehmu/Desktop/PyProjects/Projects/github repo/SatelliteWaterBodies/Water Bodies Dataset/Masks/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yüklediğimiz verinin varlığını kontrol etmemiz gerekiyor. Burada yanlış bir yol girildiğinde alttaki sonucumuz 0 çıkacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verimizi train ve validation için bölüyoruz ve üstteki sınıfımızla beraber şekillendiriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LoadData(X_train, y_train)\n",
    "valid_dataset = LoadData(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = train_dataset[0]\n",
    "\n",
    "f, axarr = plt.subplots(1,2) \n",
    "axarr[1].imshow(np.squeeze(mask.numpy()), cmap='gray')\n",
    "axarr[0].imshow(np.transpose(img.numpy(), (1,2,0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada berlirlediğimiz parametreler denemeler ile değiştirilip denenebilir fakat belirtmeden edemeyeceğim benim GPU'um(RTX 3060) bu veri setini eğitmeye başlayamadı bile. Bu yüzden denemeler yapmayı kendi bilgisayarınızda yapmanızı önermem.\n",
    "\n",
    "Bunun için Kaggle gerçekten güzel bir hizmet veriyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE='cuda'\n",
    "\n",
    "EPOCHS=35\n",
    "BATCH_SIZE=32\n",
    "LR=0.003\n",
    "\n",
    "\n",
    "ENCODER='timm-efficientnet-b0'\n",
    "WEIGHTS='imagenet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader kullanılarak modele girdileri hazırlanıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelimizi hazırlıyoruz...\n",
    "\n",
    "Burada Unet'in içine encoder'ı ve weights'ı yukarıdaki değişkenlerden belirliyoruz.\n",
    "\n",
    "Kayıp fonksiyonumuzun değeri, DiceLoss ve BCEWithLogitsLoss'tan çıkan değerlerin toplamıdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel,self).__init__()\n",
    "\n",
    "        self.arc=smp.Unet(\n",
    "            encoder_name=ENCODER,\n",
    "            encoder_weights=WEIGHTS,\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None\n",
    "        )\n",
    "    def forward(self,images,masks=None):\n",
    "        logits=self.arc(images)\n",
    "\n",
    "        if masks!=None:\n",
    "            loss1=DiceLoss(mode='binary')(logits,masks)\n",
    "            loss2=nn.BCEWithLogitsLoss()(logits,masks)\n",
    "            return logits,loss1+loss2\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelimizi derliyoruz ve 'cuda' ile çalışmasını sağlıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SegmentationModel()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eğitim döngüsünde işler daha basite indirgensin diye eğitim ve eval için kullanılacak fonksiyonu şimdiden yazıyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kayıplarımızı belirlemek için değeri ilk başta sıfırlıyoruz. Daha sonra fotoğrafları gradiyent olmadan sokup kayıpımızı hesaplıyoruz..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader,model,optimizer):\n",
    "  model.train()\n",
    "  total_loss=0.0\n",
    "  for images ,masks in tqdm(data_loader):\n",
    "    images=images.to(DEVICE)\n",
    "    masks=masks.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits,loss=model(images,masks)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss+=loss.item()\n",
    "\n",
    "    return total_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burada ise eğitim kısmına benzer bir kayıp hesabı yapıyoruz fakat buradaki önemli işlerden birisi kaybımızı direkt ele alıyor oluşumuz ve no_grad'ın izlenmesidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader,model):\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images ,masks in tqdm(data_loader):\n",
    "            images=images.to(DEVICE)\n",
    "            masks=masks.to(DEVICE)\n",
    "\n",
    "            logits,loss=model(images,masks)\n",
    "            total_loss+=loss.item()\n",
    "\n",
    "    return total_loss/len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizörünü kullanıyoruz ve eğitim döngümüzü kuruyoruz. \n",
    "\n",
    "Burada en iyi modeli kaydetmek ve kaybımızı her adımda görmek istiyoruz..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss=np.Inf\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "  train_loss=train_fn(train_loader,model,optimizer)\n",
    "  valid_loss=eval_fn(valid_loader,model)\n",
    "\n",
    "  if valid_loss< best_val_loss:\n",
    "    torch.save(model.state_dict(),'best_model.pt')\n",
    "    print('Model Saved')\n",
    "    best_valid_loss=valid_loss\n",
    "  print(f'Epochs:{i+1} Train_loss: {train_loss} Valid_loss: {valid_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7cd1a3a91448925d52e40248ab3fc9222a58eec32dc182ec3aef8fbd589352c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
