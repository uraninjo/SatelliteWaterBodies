{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code's Author: https://www.kaggle.com/code/realtimshady/water-bodies-segmentation-with-unet-and-tensorflow\n",
    "\n",
    "Burada farklı bir kişiden almamın nedeni torch ile yazılmış bir kodun nasıl tensorflow'da yazılabilmesini ele almamdır. Buradaki değişikliklere ve iki kütüphanenin farklarını inceleyerek bazı konulardaki avantaj ve fırsatlarını görmek ana amacımdır..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d franciscoescobar/satellite-images-of-water-bodies\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "for i in os.listdir('C:/Users/mehmu/Desktop/PyProjects/Projects/github repo/SatelliteWaterBodies'):\n",
    "    if '.zip' in i:\n",
    "        print(i)\n",
    "        with ZipFile(i, 'r') as zipObj:\n",
    "            zipObj.extractall()\n",
    "        os.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade opencv-contrib-python\n",
    "!pip install segmentation-models-pytorch helper\n",
    "!pip install -U git+https://github.com/albumentations-team/albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasik import ve yüklemeler yapılıyor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from skimage.filters import threshold_otsu\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow import keras\n",
    "from tensorflow.data import Dataset, AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dosyaların yollarını doğru girip kontrol etmek burada önemli olabiliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = 'C:/Users/mehmu/Desktop/PyProjects/Projects/github repo/SatelliteWaterBodies/Water Bodies Dataset/Images'\n",
    "masks_dir = 'C:/Users/mehmu/Desktop/PyProjects/Projects/github repo/SatelliteWaterBodies/Water Bodies Dataset/Masks'\n",
    "\n",
    "dirname, _, filenames = next(os.walk(images_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt kısımda normalde torch'ta küçük kodlarla yapılabilecek işleri fonksiyona çevirme işlemlerini görüyorsunuz.\n",
    "\n",
    "Her ne kadar torch öğrenmesi ve uygulaması zor gibi gözükse de bazen daha basit yerleri olabiliyor. Fakat bu kodlar saklanıp ilerideki projelerde kullanılıp işlerin kolaylaştırılması ve kütüphane gibi kullanılarak yazılanlar azaltılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_img_with_mask(image_path, images_dir: str = 'Images', masks_dir: str = 'Masks',images_extension: str = 'jpg', masks_extension: str = 'jpg') -> dict:\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "    mask_filename = tf.strings.regex_replace(image_path, images_dir, masks_dir)\n",
    "    mask_filename = tf.strings.regex_replace(mask_filename, images_extension, masks_extension)\n",
    "    mask = tf.io.read_file(mask_filename)\n",
    "    mask = tf.image.decode_image(mask, channels=1, expand_animations = False)\n",
    "    return (image, mask)\n",
    "\n",
    "@tf.function\n",
    "def resize_images(images, masks, max_image_size=2000):\n",
    "    shape = tf.shape(images)\n",
    "    scale = (tf.reduce_max(shape) // max_image_size) + 1\n",
    "    target_height, target_width = shape[-3] // scale, shape[-2] // scale\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    masks = tf.cast(masks, tf.float32)\n",
    "    if scale != 1:\n",
    "        images = tf.image.resize(images, (target_height, target_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        masks = tf.image.resize(masks, (target_height, target_width), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return (images, masks)\n",
    "\n",
    "@tf.function\n",
    "def scale_values(images, masks, mask_split_threshold = 128):\n",
    "    images = tf.math.divide(images, 255)\n",
    "    masks = tf.where(masks > mask_split_threshold, 1, 0)\n",
    "    return (images, masks)\n",
    "\n",
    "@tf.function\n",
    "def pad_images(images, masks, pad_mul=16, offset=0):\n",
    "    shape = tf.shape(images)\n",
    "    height, width = shape[-3], shape[-2]\n",
    "    target_height = height + tf.math.floormod(tf.math.negative(height), pad_mul)\n",
    "    target_width = width + tf.math.floormod(tf.math.negative(width), pad_mul)\n",
    "    images = tf.image.pad_to_bounding_box(images, offset, offset, target_height, target_width)\n",
    "    masks = tf.cast(tf.image.pad_to_bounding_box(masks, offset, offset, target_height, target_width), tf.uint8)\n",
    "    return (images, masks)\n",
    "\n",
    "@tf.function\n",
    "def augment(images, masks):\n",
    "    if np.random.choice(2):\n",
    "        images = tf.image.flip_left_right(images)\n",
    "        masks = tf.image.flip_left_right(masks)\n",
    "    if np.random.choice(2):\n",
    "        images = tf.image.flip_up_down(images)\n",
    "        masks = tf.image.flip_up_down(masks)\n",
    "    if np.random.choice(2):\n",
    "        images = tf.image.adjust_brightness(images)\n",
    "        masks = tf.image.adjust_brightness(masks)\n",
    "    if np.random.choice(2):\n",
    "        images = tf.image.adjust_contrast(images)\n",
    "        masks = tf.image.adjust_contrast(masks)\n",
    "    if np.random.choice(2):\n",
    "        images = tf.image.adjust_saturation(images)\n",
    "        masks = tf.image.adjust_saturation(masks)\n",
    "    \n",
    "\n",
    "    return (images, masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Üstteki kodlardan bahsetmek gerekirse;\n",
    "\n",
    "**load_img_with_mask**: Normal fotoğraf ve mask'i birleştirme işlemidir.\n",
    "\n",
    "**resize_images**: Fotoğrafları boyutunu istediğimiz hale getiriyoruz.\n",
    "\n",
    "**scale_values**: Değerleri 255 ile scale ediyoruz.\n",
    "\n",
    "**pad_images**: Fotoğraflara padding ekleme işlemidir.\n",
    "\n",
    "**augment**: Sağa sola, aşağı yukarı fotoğrafları döndürme işlemidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.8\n",
    "test_split = 0.2\n",
    "\n",
    "sample_list = sorted(glob('C:/Users/mehmu/Desktop/PyProjects/Projects/github repo/SatelliteWaterBodies/Water Bodies Dataset/Images/*.jpg'))\n",
    "\n",
    "n_samples = len(sample_list)\n",
    "train_split = round(train_split*n_samples)\n",
    "test_split = round(test_split*n_samples)\n",
    "\n",
    "train_list = sample_list[0:train_split]\n",
    "valid_list = sample_list[train_split:train_split+test_split]\n",
    "test_list = sample_list[train_split+test_split:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Üstte fotoğraflar yüklenip bölmek için belirlediğimiz oranda bölünüyor.\n",
    "\n",
    "Altta ise üstte belirlenen fonksiyonlar teker teker kullanılarak eğitime hazır bir şekle getiriliyor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.list_files(test_list)\n",
    "test_dataset = test_dataset.map(load_img_with_mask, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.map(scale_values, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(20)\n",
    "test_dataset = test_dataset.map(lambda img, mask: resize_images(img, mask, max_image_size=2500), num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.map(pad_images, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(1).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "validation_dataset = Dataset.list_files(valid_list)\n",
    "validation_dataset = validation_dataset.map(load_img_with_mask, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(scale_values, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.shuffle(20)\n",
    "validation_dataset = validation_dataset.map(resize_images, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(pad_images, num_parallel_calls=AUTOTUNE).cache('cache')\n",
    "validation_dataset = validation_dataset.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.batch(1).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset.list_files(train_list)\n",
    "train_dataset = train_dataset.map(load_img_with_mask, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(scale_values, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(20)\n",
    "train_dataset = train_dataset.map(resize_images, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.map(pad_images, num_parallel_calls=AUTOTUNE).cache('cache')\n",
    "train_dataset = train_dataset.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(1).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unet mimarisini kendimiz yapıyoruz. Burada torch'ta yapıldığı gibi direkt de alınabilir bir fonksiyon yazılabilirmiş."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(hidden_activation='relu', initializer='he_normal', output_activation='sigmoid'):\n",
    "    PartialConv = partial(keras.layers.Conv2D,\n",
    "        activation=hidden_activation,\n",
    "        kernel_initializer=initializer,      \n",
    "        padding='same')\n",
    "    \n",
    "    # Encoder\n",
    "    model_input = keras.layers.Input(shape=(None, None, 3))\n",
    "    enc_cov_1 = PartialConv(32, 3)(model_input)\n",
    "    enc_cov_1 = PartialConv(32, 3)(enc_cov_1)\n",
    "    enc_pool_1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_1)\n",
    "    \n",
    "    enc_cov_2 = PartialConv(64, 3)(enc_pool_1)\n",
    "    enc_cov_2 = PartialConv(64, 3)(enc_cov_2)\n",
    "    enc_pool_2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_2)\n",
    "    \n",
    "    enc_cov_3 = PartialConv(128, 3)(enc_pool_2)\n",
    "    enc_cov_3 = PartialConv(128, 3)(enc_cov_3)\n",
    "    enc_pool_3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(enc_cov_3)\n",
    "    \n",
    "    # Center\n",
    "    center_cov = PartialConv(256, 3)(enc_pool_3)\n",
    "    center_cov = PartialConv(256, 3)(center_cov)\n",
    "    \n",
    "    # Decoder\n",
    "    upsampling1 = keras.layers.UpSampling2D(size=(2, 2))(center_cov)\n",
    "    dec_up_conv_1 = PartialConv(128, 2)(upsampling1)\n",
    "    dec_merged_1 = tf.keras.layers.Concatenate(axis=3)([enc_cov_3, dec_up_conv_1])\n",
    "    dec_conv_1 = PartialConv(128, 3)(dec_merged_1)\n",
    "    dec_conv_1 = PartialConv(128, 3)(dec_conv_1)\n",
    "    \n",
    "    upsampling2 = keras.layers.UpSampling2D(size=(2, 2))(dec_conv_1)\n",
    "    dec_up_conv_2 = PartialConv(64, 2)(upsampling2)\n",
    "    dec_merged_2 = tf.keras.layers.Concatenate(axis=3)([enc_cov_2, dec_up_conv_2])\n",
    "    dec_conv_2 = PartialConv(64, 3)(dec_merged_2)\n",
    "    dec_conv_2 = PartialConv(64, 3)(dec_conv_2)\n",
    "    \n",
    "    upsampling3 = keras.layers.UpSampling2D(size=(2, 2))(dec_conv_2)\n",
    "    dec_up_conv_3 = PartialConv(32, 2)(upsampling3)\n",
    "    dec_merged_3 = tf.keras.layers.Concatenate(axis=3)([enc_cov_1, dec_up_conv_3])\n",
    "    dec_conv_3 = PartialConv(32, 3)(dec_merged_3)\n",
    "    dec_conv_3 =  PartialConv(32, 3)(dec_conv_3)\n",
    "    \n",
    "    output = keras.layers.Conv2D(1, 1, activation=output_activation)(dec_conv_3)\n",
    "    \n",
    "    return tf.keras.Model(inputs=model_input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eğitim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelimizi atıyoruz ve Nadam optimizörünü seçek compile işlemini başlatıyoruz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_unet()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*val_loss*'u izleyip değerlerine göre durdurmak için early_stopping kullanıyoruz.\n",
    "\n",
    "*lr_reduce* ile eğitim ilerledikçe learning rate'i düşürüyoruz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "lr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=1)\n",
    "\n",
    "epochs = 100\n",
    "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs, callbacks=[early_stopping, lr_reduce])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontrol ve Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OtsuFilter(prediction, bias=0):\n",
    "    threshold = threshold_otsu(prediction) - bias\n",
    "    \n",
    "    return prediction > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_examples, 3, figsize=(14, n_examples*7), constrained_layout=True)\n",
    "for ax, ele in zip(axs, test_dataset.take(n_examples)):\n",
    "    image, y_true = ele\n",
    "    prediction = model.predict(image)[0]\n",
    "    prediction = OtsuFilter(prediction)\n",
    "    ax[0].set_title('Original image')\n",
    "    ax[0].imshow(image[0])\n",
    "    ax[1].set_title('Original mask')\n",
    "    ax[1].imshow(y_true[0])\n",
    "    ax[2].set_title('Predicted area')\n",
    "    ax[2].imshow(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanIoU = MeanIoU(num_classes=2)\n",
    "for (image, y_true) in tqdm(test_dataset.take(test_split)):\n",
    "    prediction = model.predict(image)[0]\n",
    "    prediction = OtsuFilter(prediction)\n",
    "    meanIoU.update_state(y_true[0], prediction)\n",
    "print(meanIoU.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('waternet.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7cd1a3a91448925d52e40248ab3fc9222a58eec32dc182ec3aef8fbd589352c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
